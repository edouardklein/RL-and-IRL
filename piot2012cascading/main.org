#+TITLE:Learning a reward function from demonstrations: a cascaded supervised learning approach
#+OPTIONS: toc:nil
#+LaTeX_Header: \usepackage{nips12submit_e,times}
#+LaTeX_Header: \usepackage{makeidx}  % allows for indexgeneration
#+LaTeX_Header: % For figures
#+LaTeX_Header: \usepackage{graphicx} % more modern
#+LaTeX_Header: %\usepackage[latin1]{inputenc}
#+LaTeX_Header: %\usepackage[francais]{babel}
#+LaTeX_Header: \usepackage{subfigure}
#+LaTeX_Header: \usepackage{tabularx}
#+LaTeX_Header: \usepackage{mathtools}
#+LaTeX_Header: \usepackage{amsmath}
#+LaTeX_Header: \usepackage{amssymb}
#+LaTeX_Header: \usepackage{amsthm}
#+LaTeX_Header: \newtheorem{definition}{Definition}
#+LaTeX_Header: \newtheorem{theorem}{Theorem}
#+LaTeX_Header: \newtheorem{lemma}{Lemma}
#+LaTeX_Header: \newtheorem{remark}{Remark}
#+LaTeX_Header: \usepackage{dsfont}
#+LaTeX_Header: \usepackage{algorithm}
#+LaTeX_Header: \usepackage{algorithmic}
#+LaTeX_Header: \usepackage{hyperref}
#+LaTeX_Header: \hypersetup{
#+LaTeX_Header:     colorlinks,%
#+LaTeX_Header:     citecolor=black,%
#+LaTeX_Header:     filecolor=black,%
#+LaTeX_Header:     linkcolor=black,%
#+LaTeX_Header:     urlcolor=black
#+LaTeX_Header: }
#+LaTeX_Header: \mathtoolsset{showonlyrefs=true}
#+LaTeX_Header: \newtheorem{hypo}{Hypothesis}
#+LaTeX_Header: \newcommand{\argmax}{\operatorname*{argmax}}
#+LaTeX_Header: \newcommand{\argmin}{\operatorname*{argmin}}
#+LaTeX_Header: \newcommand{\arginf}{\operatorname*{arginf}}
#+LaTeX_Header: \newcommand{\minp}{\operatorname*{min_+}}
#+LaTeX_Header: \newcommand{\Ker}{\operatorname*{Ker}}
#+LaTeX_Header: \newcommand{\trace}{\operatorname*{trace}}
#+LaTeX_Header: \newcommand{\cov}{\operatorname{cov}}
#+LaTeX_Header: \newcommand{\card}{\operatorname*{Card}}
#+LaTeX_Header: \newcommand{\vect}{\operatorname*{Vect}}
#+LaTeX_Header: \newcommand{\var}{\operatorname{Var}}
#+LaTeX_Header: \newcommand{\diag}{\operatorname{diag}}
#+LaTeX_Header: \newcommand{\erf}{\operatorname{erf}}
#+LaTeX_Header: \newcommand{\bound}{\operatorname*{bound}}
#+LaTeX_Header: \newcommand{\vpi}{\operatorname{VPI}}
#+LaTeX_Header: \newcommand{\gn}{\operatorname{Gain}}
#+LaTeX_Header: \newcommand{\p}{\operatorname{Pr}}
#+LaTeX_Header: \newcommand{\mlp}{\operatorname{MLP}}
#+LaTeX_Header: \newcommand*\tto[2]{\smash{\mathop{\longrightarrow}\limits_{#1}^{#2}}}
#+LaTeX_Header: \newcommand*\ntto[2]{\smash{\mathop{\nrightarrow}\limits_{#1}^{#2}}}
#+LaTeX_Header: \newcommand{\X}{\mathbf{X}}
#+LaTeX_Header: \newcommand{\Q}{\mathbf{Q}}
#+LaTeX_Header: \newcommand{\A}{\mathbf{A}}
#+LaTeX_Header: \newcommand{\Z}{\mathbf{Z}}
#+LaTeX_Header: \newcommand{\Y}{\mathbf{Y}}
#+LaTeX_Header: \newcommand{\E}{\mathbf{E}}
#+LaTeX_Header: \newcommand{\K}{\mathbf{K}}
#+LaTeX_Header: \newcommand{\F}{\mathcal{F}}
#+LaTeX_Header: \newcommand{\R}{\mathbf{R}}
#+LaTeX_Header: \newcommand{\ba}{\mathbf{a}}
#+LaTeX_Header: \newcommand{\bb}{\mathbf{b}}
#+LaTeX_Header: \newcommand{\bc}{\mathbf{c}}
#+LaTeX_Header: \newcommand{\bd}{\mathbf{d}}
#+LaTeX_Header: \newcommand{\be}{\mathbf{e}}
#+LaTeX_Header: \newcommand{\af}{\mathbf{f}}
#+LaTeX_Header: \newcommand{\bg}{\mathbf{g}}
#+LaTeX_Header: \newcommand{\bh}{\mathbf{h}}
#+LaTeX_Header: \newcommand{\bi}{\mathbf{i}}
#+LaTeX_Header: \newcommand{\bj}{\mathbf{j}}
#+LaTeX_Header: \newcommand{\bk}{\mathbf{k}}
#+LaTeX_Header: \newcommand{\bl}{\mathbf{l}}
#+LaTeX_Header: \newcommand{\bm}{\mathbf{m}}
#+LaTeX_Header: \newcommand{\bn}{\mathbf{n}}
#+LaTeX_Header: \newcommand{\bo}{\mathbf{o}}
#+LaTeX_Header: \newcommand{\bp}{\mathbf{p}}
#+LaTeX_Header: \newcommand{\bq}{\mathbf{q}}
#+LaTeX_Header: \newcommand{\br}{\mathbf{r}}
#+LaTeX_Header: \newcommand{\bs}{\mathbf{s}}
#+LaTeX_Header: \newcommand{\bt}{\mathbf{t}}
#+LaTeX_Header: \newcommand{\bu}{\mathbf{u}}
#+LaTeX_Header: \newcommand{\bv}{\mathbf{v}}
#+LaTeX_Header: \newcommand{\bw}{\mathbf{w}}
#+LaTeX_Header: \newcommand{\bx}{\mathbf{x}}
#+LaTeX_Header: \newcommand{\by}{\mathbf{y}}
#+LaTeX_Header: \newcommand{\bz}{\mathbf{z}}
#+LaTeX_Header: \newcommand{\ma}{\mathbf{A}}
#+LaTeX_Header: \newcommand{\mb}{\mathbf{B}}
#+LaTeX_Header: \newcommand{\mc}{\mathbf{C}}
#+LaTeX_Header: \newcommand{\md}{\mathbf{D}}
#+LaTeX_Header: \newcommand{\me}{\mathbf{E}}
#+LaTeX_Header: \newcommand{\mf}{\mathbf{F}}
#+LaTeX_Header: \newcommand{\mg}{\mathbf{G}}
#+LaTeX_Header: \newcommand{\mh}{\mathbf{H}}
#+LaTeX_Header: \newcommand{\mi}{\mathbf{I}}
#+LaTeX_Header: \newcommand{\mj}{\mathbf{J}}
#+LaTeX_Header: \newcommand{\mk}{\mathbf{K}}
#+LaTeX_Header: \newcommand{\ml}{\mathbf{L}}
#+LaTeX_Header: \newcommand{\mm}{\mathbf{M}}
#+LaTeX_Header: \newcommand{\mn}{\mathbf{N}}
#+LaTeX_Header: \newcommand{\mo}{\mathbf{O}}
#+LaTeX_Header: \newcommand{\Mp}{\mathbf{P}}
#+LaTeX_Header: \newcommand{\mq}{\mathbf{Q}}
#+LaTeX_Header: \newcommand{\mr}{\mathbf{R}}
#+LaTeX_Header: \newcommand{\ms}{\mathbf{S}}
#+LaTeX_Header: \newcommand{\mt}{\mathbf{T}}
#+LaTeX_Header: \newcommand{\Mu}{\mathbf{U}}
#+LaTeX_Header: \newcommand{\mv}{\mathbf{V}}
#+LaTeX_Header: \newcommand{\mw}{\mathbf{W}}
#+LaTeX_Header: \newcommand{\mx}{\mathbf{X}}
#+LaTeX_Header: \newcommand{\my}{\mathbf{Y}}
#+LaTeX_Header: \newcommand{\mz}{\mathbf{Z}}
#+LaTeX_Header: \newcommand{\tphi}{\tilde{\Phi}}
#+LaTeX_Header: \newcommand{\espace}{\text{ }}
#+LaTeX_Header: \newcommand{\x}{\mathbf{x}}
#+LaTeX_Header: \newcommand{\s}{\mathbf{s}}
#+LaTeX_Header: \newcommand{\n}{\mathbf{n}}
#+LaTeX_Header: \newcommand{\y}{\mathbf{y}}
#+LaTeX_Header: \newcommand{\I}{\mathbf{I}}
#+LaTeX_Header: \newcommand{\rr}{\mathbf{r}}
#+LaTeX_Header: \newcommand{\0}{\mathbf{0}}
#+LaTeX_Header: \newcommand{\1}{\mathbf{1}}
#+LaTeX_Header: \newcommand{\am}{{\mathcal{A}_m}}
#+LaTeX_Header: \newcommand{\amj}{{\mathcal{A}_m^{+j}}}
#+LaTeX_Header: \newcommand{\sgn}{\operatorname{sgn}}
#+LaTeX_Header: \title{Learning a reward function from demonstrations: a cascaded supervised
#+LaTeX_Header: learning approach}
#+LaTeX_Header: \author{Edouard Klein$^{1,2}$\\
#+LaTeX_Header:  1. ABC Team\\
#+LaTeX_Header:  LORIA-CNRS, France.
#+LaTeX_Header: \And Bilal Piot$^{2}$\\
#+LaTeX_Header:  2. Supélec-Metz Campus\\
#+LaTeX_Header:  MaLIS Research group, France\\
#+LaTeX_Header: \And Matthieu Geist$^1$\\
#+LaTeX_Header: \texttt{prenom.nom@supelec.fr}\\
#+LaTeX_Header: \And Olivier Pietquin$^{2,3}$\\
#+LaTeX_Header: 3. UMI 2958 CNRS\\
#+LaTeX_Header: GeorgiaTech, France
#+LaTeX_Header: }
#+LaTeX_Header: 
#+LaTeX_Header: % The \author macro works with any number of authors. There are two commands
#+LaTeX_Header: % used to separate the names and addresses of multiple authors: \And and \AND.
#+LaTeX_Header: %
#+LaTeX_Header: % Using \And between authors leaves it to \LaTeX{} to determine where to break
#+LaTeX_Header: % the lines. Using \AND forces a linebreak at that point. So, if \LaTeX{}
#+LaTeX_Header: % puts 3 of 4 authors names on the first line, and the last on the second
#+LaTeX_Header: % line, try using \AND instead of \And before the third author name.
#+LaTeX_Header: 
#+LaTeX_Header: \newcommand{\fix}{\marginpar{FIX}}
#+LaTeX_Header: \newcommand{\new}{\marginpar{NEW}}
#+LaTeX_Header: 
#+LaTeX_Header: 


#+begin_abstract
The IRL problem, that is dicovering the reward function optimized by an expert in an MDP, is addressed in this paper. The proposed generic, model-free contribution consists in cascading a classification and a regression steps to produce a non-trivial reward function. We prove the expert policy to be near optimal with respect to this reward. The proposed algorithm does not require solving any direct reinforcement learning problem (unlike most of IRL algorithms) and, with the help of some heuristics, can be instanciated to solely rely on transitions from the expert (no need to sample trajectories according to other policies). All of this is illustrated through a standard benchmark.
#Peut etre mettre benchmark au pluriel si on en met plusieurs
#+end_abstract
* Problem statement
#+begin_comment
  - [X] RL is getting a policy from a reward
  - [X] But defining a good reward can be difficult
  - [X] An expert that intuitively optimizes a good reward may provide a solution to this problem
  - [X] One can try to imitate an expert. Some people call it Learnign from demonstration.
  - [X] One can more precisely try to imitate the expert's policy in an MDP (apprenticeship learning)
  - [X] We do IRL because we want to extract the reward function (biological or economicakl nspiration, succinct description of a task, transfer learning)
  - [X] IRL has been seen as a way to do apprenticeshipe learning
  - [X] Our algorithm begins like an apprenticeship learning algorithm by using a score function based classifier to imitate the expert
  - [X] But we introduce the structure of the MDP in a second supervised learning step, namely a regression step.
  - [X] The whole algorithm has some interesting properties (better than others)
  - [X] The reward has some properties (analysis)
  - [X] With some heuristics as illustrated in the experiment, we even have more properties (better than others)
This reward function is seen as the most succinct description of the task, allowing for task transfer from the expert to an agent with potentially different abilities. 
#+end_comment

Given a sequential decision making problem framed in the Markov Decision Process (MDP) formalism, it is often difficult to manually specify a reward function in order to train an agent to fulfill a certain task. However, observing an expert demonstrating this task is often possible and easier \cite{russell1998learning}. That is why learning from demonstrations and more particularly Inverse Reinforcement Learning (IRL) has received more and more interest this last decade. The IRL framework, first introduced in \cite{russell1998learning,ng2000algorithms}, assumes that an expert, demonstrating a task, is acting optimally in an MDP with respect to an unknown reward function to be discovered. Unlike direct methods of apprenticeship learning[fn:: The definition of apprenticeship learning we use is a restriction of learning from demonstration to MDP settings, where the output of the algorithm is a control policy.] which learn a mapping from states to actions via supervised learning (SL) \cite{atkeson1997robot,pomerleau1989alvinn}, IRL methods aim at recovering a reward function. We see this reward function as the most compact and transferable way to specify a task. One can even try to derive semantic information from it, for example in biological or economical studies as hinted to in \cite{russell1998learning}.
Most of the direct methods do not use the structure of the MDP (one notable exception being \cite{melo2010learning}), hence only mimic the policy of the expert and are not able to generalize it correctly, possibly leading to incorrect behavior outside demonstrated states.
In order to overcome this drawback, undirect methods using the IRL framework \cite{abbeel2004apprenticeship} were introduced. Most of these methods (see \cite{neu2009training} for a comprehensive overview) assume the dynamic of the environment known, or need an environment model (via a simulator) so as to test the effects of a policy. Moreover, most of existing undirect approaches require to solve the forward Reinforcement Learning (RL) problem (find an optimal policy knowing a given reward) several times (except  \cite{boularias2011relative} which however requires sampling non expert trajectories).
Finally, in many cases, the output of the algorithm is a policy but not a reward function (see Section \ref{section: related work} for an in-depth presentation). These constraints can even lead to difficulties in real-world applications and make the problem of apprenticeship learning harder than the forward RL problem. That is why a reborn interest for direct methods which passively imitate the expert exists. More particularly the reduction of the apprenticeship learning to classification, first introduced 
#VERIFIER CETTE REF
in \cite{zadrozny2003cost}
and recently used 
#Est-ce qu'on rajoute ratliff ?
in \cite{melo2010learning}, is legitimated for finite horizons problems in \cite{syed2010reduction},\cite{ross2010efficient}.
Our approach tries to avoid the drawbacks of both direct and undirect methods by cascading two well-known supervised learning methods (see Section \ref{section: Cascading}): first a step of classification from data drawn by the expert provides us with a score function over the state-action space (i.e. for each state we are provided with a ranking of the actions), then a step of regression introduces the MDP structure and outputs a reward function.
Our approach doesn't require any of the following: complete trajectories from an expert, a generative model of the environment, the knowledge of the transition probabilities, the ability to compute a (near)-optimal policy for different rewards and hence solve the forward (RL) problem or even the perfect knowledge of the expert's policy. Also, we exhibit a reward function and not a control policy. Our analysis (Section \ref{section: Analysis}) show that the expert is epsilon-optimal with respect to the resulting reward function. Moreover, our algorithm can be instanciated with any off-the-shelf score-function based classifier and any regression method. We provide such an instanciation and empirically show (Section \ref{section: experiments}) that only a small amount of expert demonstrations (not even in the form of trajectories but simple transitions) is required. Comparison to other state-of-the-art methods is provided Section \ref{section: related work}. We start with somebackground mathematical notations in Section \ref{section: background}.

* Background and notations
  \label{section: background}
First, we introduce some general notations. Let $X=\{x_i\}_{1\leq i \leq I}$ be a finite set of $I\in\mathbb{N}^*$ elements, a function $g\in\mathbb{R}^X$ can also be seen as the real column vector $g=[g(i)=g(x_i)]_{1\leq i \leq I}$, $g^T$ is the transposition of $g$, and $|g|=[|g|(i)=|g(i)|]_{1\leq i \leq I}$.
$\Delta_X$ is the set of probability distributions over $X$. Let $\mu\in\Delta_X$ and $f\in\mathbb{R}^X$, we define the $\mu$-weighted $l_1$-norm:
\begin{equation}
\|f\|_{\mu,1}=\sum_{x\in X}\mu(x)|f(x)|=E_{x\sim\mu}[|f(x)|]=\mu^T|f|,
\end{equation}
where the notation $E_{x\sim\mu}$ is the expectation with $x$ drawn according to $\mu$.

Now, we introduce notations specific to the MDP paradigm. A (finite) MDP \cite{puterman1994markov} is a tuple $M=\{S,A,P,\gamma,R\}$ where $S=\{s_i\}_{1\leq i \leq N}$ is the finite state space with $N\in\mathbb{N}^*$ states, $A=\{a_k\}_{1\leq k \leq K}$ is the finite action space with $K\in\mathbb{N}^*$ actions, $P=\{P_{s,a}\}_{(s,a)\in S\times A}$ is the set of transition probabilities where $P_{s,a}$ is a distribution probability over $S$. $P$ represents the dynamics of the MDP and the notation $p(s,a,s')=P_{s,a}(s')$ quantifies the probability to reach $s'\in S$ knowing that the action $a \in A$ was taken in the state $s\in S$, $\gamma\in]0,1[$ is the discount factor and $R$ is a function from $S\times A$ to $\mathbb{R}$ called the reward function. A deterministic policy $\pi$ is a function from $S$ to $A$, the value function $V^\pi_R$ is a function from $S$ to $\mathbb{R}$ defined by:
\begin{equation}
V^\pi_R(s)=E^\pi_s[\sum_{t=0}^{+\infty}\gamma^tR(s_t,\pi(s_t))], \forall s \in S,
\end{equation}
where $E^\pi_s$ is the expectation over the distribution of the trajectories $(s_0,s_1,\dots)$ obtained by executing the policy $\pi$ starting from $s_0=s$.
The action-value function $Q^\pi_R$ is a function from $S\times A$ to $R$ defined by: $\forall s\in S, \forall a\in A$, $Q^{\pi}_R(s,a)=R(s,a)+\gamma\sum_{s'\in S}P_{s,a}(s')V^{\pi}_R(s')$. The classical result for finite MDPs is the well-known Bellman equation: $\forall s\in S$, $V^{\pi}_R(s)~=~R(s,\pi(s))~+~\gamma\sum_{s'\in S}P_{s,\pi(s)}(s')V^{\pi}_R(s')$.
A policy $\pi$ is said optimal for the reward $R$ when:
$\forall s\in S, \forall \tilde{\pi}\in A^{S}, V^{\pi}_R(s)\geq V^{\tilde{\pi}}_R(s)$.
Another classical result called the Bellman optimality is: $\pi$ is an optimal policy for the reward $R$ if and only if:
\begin{equation}
\label{equation:Qoptimal}
\forall s\in S, \pi(s)\in\argmax_{a\in A} Q^\pi_R(s,a).
\end{equation}
For a given policy $\pi$ and a given reward function $R$, we define $R_\pi=[R_\pi(i)=R(s_i,\pi(s_i))]_{1\leq i \leq N}$, the $N\times N$-matrix $P_\pi=[P_\pi(i,j)=p(s_i,\pi(s_i),s_j]_{1\leq i,j \leq N}$ and the Bellman operator $T^\pi_R$ which is a function from $\mathbb{R}^S$ to $\mathbb{R}^S$:
\begin{equation}
\forall V\in\mathbb{R}^S, T^\pi_RV=R_\pi+\gamma P_\pi V.
\end{equation}
It is well known that the unique fixed point of the operator $T^\pi_R$ is  $V^\pi_R$ \cite{puterman1994markov}.\\
RL consists in finding an optimal policy for the reward function $R$. Notice that the state space may be too large for an exact representation of the value function (which calls for approximate
representation), that the model ($P$ and $R$) may be unknown (the only information being provided through rewarded transitions sampled according
to some behavioral policy), that learning can occur in an online or off-line setting, and so on. See \cite{bertsekas2001dynamic},\cite{sutton1998reinforcement} for details.

In the classical IRL paradigm \cite{ng2000algorithms}, an MDP without reward $M\backslash R =\{S,A,P,\gamma\}$ and a policy $\pi_E$ called expert-policy are given and the problem is to find
a reward $R^*$ for which the policy $\pi_E$ is optimal. However this problem is clearly ill-posed in the sense that there is not uniqueness of the reward $R^*$: many reward functions are equivalent in that they have the same optimal deterministic policies, moreover the trivial zero-reward is a solution for any deterministic policy $\pi_E$ as it is shown in \cite{ng2000algorithms}. In the literature, some solutions are proposed in order to respond to the ill-posed nature of the problem \cite{ng2000algorithms,ziebart2008maximum,boularias2011relative}.
In our experiments (see Section \ref{section: experiments}) we assume that the solely available information is provided by transitions sampled according to the dynamics of the environnement under $\pi_E$: $\{(s_i,a_i=\pi_E(s_i)),s_i')\}_{1\leq i \leq D}, D\in\mathbb{N}^*$,
where $s_i'$ is sampled according to the distribution $P_{s_i,a_i}$.
The reward function is obviously unknown, but this assumption means that the dynamics ($P$) is only known through transitions $(s_i, a_i, s_i')$ and that the
policy $\pi_E$ is only known through state-action pairs $(s_i, a_i)$.\\
For a given deterministic policy $\pi$, a component $P_{s_i,\pi(s_i)}(s_j)$ of the matrix $P_\pi$ represents the probability to transit from $s_i$ to $s_j$ under the policy $\pi$. So $P_\pi$ can be seen as a transition matrix of a finite Markov-chain on the finite state space $S$. If $P_\pi$ is irreducible then it exists a unique distribution $\mu_\pi$ (see \cite{baldi2000martingales}) called the stationary distribution such that:
\begin{equation}
\label{equation: stationarity}
\mu_\pi^T=\mu_\pi^T P_\pi.
\end{equation}
\label{section: background}
* Cascading Classification and Regression for IRL
\label{section: Cascading}
#Probleme sur les refs
# rajouter les refs quand on présente les différents types de classifieurs
#+begin_comment
    - [X] Data set
    - [X] Decision rule
      - [X] Examples
    - [X] pi_C
#+end_comment
The first SL step in our algorithm is to train a classifer over a set 
\begin{equation}
\label{equation:data}
D_C=\{(s_i,a_i=\pi_E(s_i)),s'_i)\}_{1\leq i \leq D}, D\in\mathbb{N}^*,
\end{equation} where $s'_{i}$ is sampled according to the distribution $P_{s_{i},a_{i}}$. The actions $a_i$ are seen as labels for the inputs $s_i$. We restrict ourselves to the use of score-function based classifiers for which the classification rule is of the form $\forall s \in S, \pi_C(s) \in \arg\max_{a\in A} q(s,a)$. Most classifiers, from $k$-nearest neighboors to multi-class-SVMs and structured margin approaches, fall into this category. The classification rule of this broad range of classifiers can be seen as a deterministic policy. After noticing the similarity between the definition of $\pi_C$ and equation \eqref{equation:Qoptimal}, it is easy and natural to identify the score function $q:S\times A \rightarrow \mathbb{R}$ with a state-action value function with respect to a certain reward function $R^C$ for which $\pi_C$ is an optimal policy (explained and proven in Section \ref{section: Analysis}).\\
As the model is often unknown, it is not possible to directly compute $R^C$. We can however use a new dataset $D_R=\{(s_{i},a_{i},s'_{i})\}_{1\leq i \leq D'}, D'\in\mathbb{N}^*$, where $s'_{i}$ is sampled under the probability $P_{s_{i},a_{i}}$. In $D_R$, we don't necessarily have $a_i = \pi_E(s_i)$ as we had in dataset $D_C$. We can write :
\begin{equation}
\label{ri.def}
 \forall i\in \{1,\dots,D'\},\hat{r}_i=q(s_{i},a_{i})-\gamma q(s'_{i},\pi_C(s'_{i})).
\end{equation}
Therefore, it is possible to build an estimate $\hat{R}^C$, which is a function from $S\times A$ to $\mathbb{R}$, of $R^C$ using a regressor trained on the dataset:$\{(s_{i},a_{i},\hat{r}_i)\}_{1\leq i \leq D'}$.

The whole approach is summed up algorithm \ref{algo:cascading}. We see $\hat{R}^C$ as an approximation of $R^C$ and 
#Nécessaire de définir pi_C ?
$\hat\pi_{C}$ is defined as an optimal policy for the reward $\hat{R}^C$.
#
In order to verify that the reward function $\hat{R}^C$ is a good candidate to resolve the IRL problem, it will be proven in Section \ref{section: Analysis} that the policy $\pi_E$ is near-optimal for the reward $\hat{R}^C$ (confer theorem \ref{theorem : results}).

\begin{algorithm}%[H]
    %\small
  \caption{Cascading IRL algorithm}
  \label{algo:cascading}
  \emph{\textbf{Given}} a training set $D_C=\{(s_i,a_i=\pi_E(s_i)),s'_i)\}_{1\leq i \leq D}$ and another training set $D_R=\{(s_{i},a_{i},s'_{i})\}_{1\leq i \leq D'}$\;\\
  \emph{\textbf{Train}} a score function-based classifier on $D_C$, obtaining decision rule $\pi_C$ and score function $q:S\times A \rightarrow \mathbb R$\;\\
  \emph{\textbf{Learn}} a reward function $\hat R^C$ from the dataset $\{(s_{i},a_{i},\hat{r}_i)\}_{1\leq i \leq D'}$, $\forall (s_i,a_i,s'_i) \in D_R,\hat{r}_i=q(s_{i},a_{i})-\gamma q(s'_{i},\pi_C(s'_{i}))$\;\\
  \emph{\textbf{Output}} the reward function $\hat R^{C}$ \;
\end{algorithm}

#+begin_comment
    - [X] R_C
    - [ ] Injection eq.1
    - [ ] ^r_i
    - [ ] min_i r_i (heuristics)
    - [ ] complete algorithm
#+end_comment
* Analysis
\label{section: Analysis}
This section is devoted to show, under some hypotheses, that the cascading approach is legitimate. Lemma \ref{lemma: calculs V}  gives a practical way to calculate $E_{s\sim\mu_\pi}[V^\pi_R]$ for a given policy $\pi$ and a given reward function $R$. Theorem \ref{theorem : results}  gives an upper bound to the term $E_{s\sim\mu_E}[V^{\hat{\pi}_C}_{\hat{R}^C}-V^{\pi_E}_{\hat{R}^C}]$ where $\mu_E$ is the stationary distribution of the expert policy $\pi_E$. We also give an interpretation of the term $E_{s\sim\mu_E}[V^{\hat{\pi}_C}_{\hat{R}^C}-V^{\pi_E}_{\hat{R}^C}]$ and explain why being able to bound this term means that our approach is legitimate.
\subsection{Results and Discussion}
\begin{lemma}
\label{lemma: calculs V}
Let $\{S,A,P,\gamma,R\}$ be a finite MDP and $\pi$ a deterministic policy.
If $P_\pi$ is irreducible, then $E_{s\sim\mu_\pi}[V^\pi_R]=\mu_\pi^TV^\pi_R=\frac{1}{1-\gamma}\mu_\pi^TR_\pi$.
\end{lemma}
Lemma \ref{lemma: calculs V} gives a practical tool which will be useful in order to simplify some terms in the proof of Theorem \ref{theorem : results}.
Before giving an upper bound to $\E_{s\sim\mu_E}[V^{\hat{\pi}_C}_{\hat{R}^C}-V^{\pi_E}_{\hat{R}^C}]$, we define $\epsilon_C\in\mathbb{R}_+$ called the classification error and the function $\epsilon_R$ from $S\times A$ to $\mathbb{R}$ called the regression error such that:
\begin{align}
&\epsilon_C=\sum_{s\in S}\mu_{E}(s)\mathds{1}_{\{\pi_C(s)\neq\pi_E(s)\}}=E_{s \sim \mu_E}[\mathds{1}_{\{\pi_C(s)\neq\pi_E(s)\}}],
\\
&\forall a\in A, \forall s\in S, \epsilon_R(s,a)=\hat{R}^C(s,a)-R^C(s,a).
\end{align}
We define $\epsilon^C_R=[\epsilon_R(s_i,\hat{\pi}_C(s_i))]_{1\leq i \leq N}$ and $\epsilon^E_R=[\epsilon_R(s_i,\pi_E(s_i))]_{1\leq i \leq N}$.
\begin{remark}
One can notice that the classification error is defined thanks to the expectation $E_{s \sim \mu_E}$. In the classical framework of classification, the data $\{s_i,a_i=\pi_E(s_i)\}_{1\leq i \leq D}$ are generated independently according to a distribution $\mu_{\text{Data}}$ over $S$ and hence the classical classification error must be $\epsilon_C=E_{s \sim \mu_{\text{Data}}}[\mathds{1}_{\{\pi_C(s)\neq\pi_E(s)\}}]$.
Then one can suppose that our data $\{s_i,a_i=\pi_E(s_i)\}_{1\leq i \leq D}$ are generated independently thanks to $\mu_E$ but this assumption is quite strong and non-realistic.
However it is often the case that the data provided by the expert are trajectories. In that case ,if $P_{\pi_E}$ is irreducible , the rate of convergence of the data distribution
to the stationary distribution is at least exponential (this is the Doeblin theorem see \cite{baldi2000martingales}). It allows us to suppose that the data $\{s_i,a_i=\pi_E(s_i)\}_{1\leq i \leq D}$ are generated under $\mu_E$.
\end{remark}
Finally, we introduce also the first order discounted future state distribution concentration coefficient (\cite{MunosSIAM07}):
\begin{equation}
C_1=(1-\gamma)\sum_{t\geq0}\gamma^tc(t), \text{ with } c(t)=\max_{\pi_1,\dots,\pi_t,s}\frac{(\mu_E^TP_{\pi_1}\dots P_{\pi_t})(s)}{\mu_E(s)}.
\end{equation}
\begin{theorem}
\label{theorem : results}
Let $\{S,A,P,\gamma\}$ be a finite MDP without reward and $\pi_E$ an expert policy.
The notations $q$, $\pi_C$, $\hat{\pi}_C$, $\hat{R}^C$ are introduced in the Section \ref{section: Cascading}.
If $P_{\pi_E}$ is irreducible, then $\mu_E$ is the stationary distribution of $\pi_E$ and:
\begin{enumerate}
\item $\pi_C$ is optimal for the reward $R^C$.
\item $0\leq E_{s\sim\mu_E}[V^{\pi_C}_{R^C}(s)-V^{\pi_E}_{R^C}(s)]\leq\frac{\epsilon_C\max_{s\in S}(q(s,\pi_C(s))-\min_{a\in A}q(s,a))}{1-\gamma}$.
\item $0 \leq E_{s\sim\mu_E}[V^{\hat{\pi}_C}_{\hat{R}^C}(s)-V^{\pi_E}_{\hat{R}^C}(s)]\leq \frac{\epsilon_C\max_{s\in S}(q(s,\pi_C(s))-\min_{a\in A}q(s,a))+\|\epsilon^E_R\|_{\mu_E,1}+C_1\|\epsilon^C_R\|_{\mu_E,1}}{1-\gamma}$.
\end{enumerate}
\end{theorem}
In order to understand why this theorem is useful, let us make some important assumptions. Let us suppose that the classification and the regression steps are perfect in the sense that $\forall (s,a)\in S\times A,\epsilon_R(s,a)=0$ and $\epsilon_C=0$. Then we obviously have, thanks to the theorem \ref{theorem : results}, that $\pi_E=\pi_C$ is optimal for $\hat{R}^C=R^C$. Thus the method is able to provide a non-trivial reward function for which the policy $\pi_E$ is optimal. Moreover if the classification step and the regression step are not perfect, the theorem \ref{theorem : results} shows, that our approach is able to provide a non trivial-reward $\hat{R}^C$ for which the policy $\pi_E$ is near-optimal in the sense that:
\begin{equation}
0 \leq E_{s \sim \mu_E}[V^{\hat{\pi}_C}_{\hat{R}^C}(s)-V^{\pi_E}_{\hat{R}^C}(s)]\leq \mathcal{O}(\frac{\epsilon_C+ \|\epsilon^C_R\|_{\mu_E,1}+\|\epsilon^E_R\|_{\mu_E,1}}{1-\gamma})
\end{equation}
\begin{remark}
It is important to be clear about this result. If the only available data are provided by equation \eqref{equation:data}, it is possible to control
$\epsilon_C$ and $\|\epsilon^E_R\|_{\mu_E,1}$ because these errors depend only on the expert policy $\pi_E$. However it is not possible to control the error $\|\epsilon^C_R\|_{\mu_E,1}$
because it depends on the policy $\hat{\pi}_C$ which can be different from the expert policy and hence do not appear in the available data \eqref{equation:data}. However
it will be possible to obtain a control on the term $\|\epsilon^C_R\|_{\mu_E,1}$ if the data used for the regression are $D_R=\{(s_i,a_i,s'_i)\}_{1\leq i \leq D'}$, where $(s_{i},a_{i})$ are uniformly chosen on the set $S\times A$ or sampled from other policies than the expert. So, theoretically an easy way to be sure
to control the error $\|\epsilon^C_R\|_{\mu_E,1}$ is to be able to give a data set for the regression which is sampled from the expert policy and other policies (and more particularly $\hat{\pi}_C$). But we give examples, see Section \ref{section: experiments}, where the regression data set given by the equation \eqref{equation:data} is sufficient to obtain good results.
A possible argument to explain the fact that classification-regression still works when $D_C=D_R$, is that $\hat{\pi}_C$ must not be so different than $\pi_E$. 
\end{remark}
\subsection{Proofs}
\begin{proof}[Lemma \ref{lemma: calculs V}]
Here, we use equation \eqref{equation: stationarity}
\begin{align}
E_{s\sim\mu_\pi}[V^\pi_R]=\mu_\pi^TV^\pi_R&=\mu_\pi^T(R_\pi+\gamma P_\pi V^\pi_R)=\mu_\pi^TR_\pi+ \gamma\mu_\pi^TP_\pi V^\pi_R,
\\
&=\mu_\pi^TR_\pi+ \gamma\mu_\pi^TV^\pi_R=\frac{1}{1-\gamma}\mu_\pi^TR_\pi.
\end{align}
\end{proof}
\begin{proof}[Theorem \ref{theorem : results}]
In order to prove the three results of the theorem \ref{theorem : results}, let us introduce the function $R_E$ from $S\times A$ to $\mathbb{R}$ such that:
\begin{equation}
\forall a \in A, \forall s\in S, R^E(s,a)=q(s,a)-\gamma\sum_{s'\in S}p(s,a,s')q(s',\pi_E(s')).
\end{equation}
The first step is to show that: $\forall s\in S, q(s,\pi_C(s))=V^{\pi_C}_{R^C}(s)$ and $\forall s\in S, q(s,\pi_E(s))=V^{\pi_E}_{R^E}(s)$.

This is quite straightforward because $q_{\pi_E}=[q(s,\pi_E(s))]_{1\leq i\leq N}$ is the fixed point of the operator $T^{\pi_E}_{R^E}$ and  $q_{\pi_C}=[q(s,\pi_C(s))]_{1\leq i\leq N}$ is the fixed point of the operator $T^{\pi_C}_{R^C}$:
\begin{align}
T^{\pi_E}_{R^E}(q_{\pi_E})&=R^E_{\pi_E}+\gamma P_{\pi_E}q_{\pi_E},
\\
&=q_{\pi_E}-\gamma P_{\pi_E}q_{\pi_E}+\gamma P_{\pi_E}q_{\pi_E}=q_{\pi_E}.
\end{align}
With the same calculus, we show that $T^{\pi_C}_{R^C}(q_{\pi_C})=q_{\pi_E}$.
Moreover it is clear that $\forall s\in S,\forall a\in A, q(s,a)=Q^{\pi_C}_{R^C}(s,a)$ and as $R^C(s,a)=q(s,a)-\gamma\sum_{s'\in S}p(s,a,s')q(s',\pi_C(s'))$:
\begin{align}
\forall s\in S,\forall a\in A, Q^{\pi_C}_{R^C}(s,a)&=R^C(s,a)+\gamma\sum_{s'\in S}p(s,a,s')V^{\pi_C}_{R^C}(s'),
\\
&=R^C(s,a)+\gamma\sum_{s'\in S}p(s,a,s')q(s',\pi_C(s'))
\\
&=q(s,a).
\end{align}
So $\forall s\in S,\forall a\in A, q(s,a)=Q^{\pi_C}_{R^C}(s,a)$ and as $\forall s\in S, \pi_C(s)\in\argmax_{a\in A}q(s,a)$, $\pi_C$ is optimal for the reward $R^C$ via equation \eqref{equation:Qoptimal}, which implies $E_{s\sim\mu_E}[V^{\pi_C}_{R^C}(s)-V^{\pi_E}_{R^C}(s)] \geq 0$.
Now let us prove that:
\begin{equation}
E_{s\sim\mu_E}[V^{\pi_C}_{R^C}(s)-V^{\pi_E}_{R^C}(s)]=\mu_E^T(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^C})\leq\frac{\epsilon_C\max_{s\in S}(q(s,\pi_C(s))-\min_{a\in A}q(s,a))}{1-\gamma}.
\end{equation}
Indeed:
\begin{equation}
\mu_E^T(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^C})=\mu_E^T(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^E}+V^{\pi_E}_{R^E}-V^{\pi_E}_{R^C}).
\end{equation}
And $\mu_E^T(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^E})$ is such that:
\begin{align}
\mu_E^T(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^E})&=\sum_{s\in S}\mu_E(s)[V^{\pi_C}_{R^C}(s)-V^{\pi_E}_{R^E}(s)],
\\
&=\sum_{s\in S}\mu_E(s)[q(s,\pi_C(s))-q(s,\pi_E(s))]\mathds{1}_{\{\pi_C(s)\neq\pi_E(s)\}},
\\
&\leq\epsilon_C\max_{s\in S}(q(s,\pi_C(s))-\min_{a\in A}q(s,a)).
\end{align}
It remains to deal with the term $\mu_E^T(V^{\pi_E}_{R^E}-V^{\pi_E}_{R^C})$ using the fact that $\mu_E^TP_{\pi_E}=\mu_E^T$ and Lemma \ref{lemma: calculs V}:
\begin{align}
\mu_E^T(V^{\pi_E}_{R^E}-V^{\pi_E}_{R^C})&=\frac{1}{1-\gamma}\mu_E^T(R^E_{\pi_E}-R^C_{\pi_E}),
\\
&=\frac{1}{1-\gamma}\mu_E^T(\gamma P_{\pi_E}q_{\pi_C}-\gamma P_{\pi_E}q_{\pi_E}),
\\
&=\frac{\gamma}{1-\gamma}\sum_{s\in S}\mu_E(s)[q(s,\pi_C(s))-q(s,\pi_E(s))]\mathds{1}_{\{\pi_C(s)\neq\pi_E(s)\}},
\\
&\leq\frac{\gamma}{1-\gamma}\epsilon_C\max_{s\in S}(q(s,\pi_C(s))-\min_{a\in A}q(s,a)).
\end{align}
Finally:
\begin{equation}
\mu_E^T(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^C})=\mu_E^T(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^E}+V^{\pi_E}_{R^E}-V^{\pi_E}_{R^C})\leq\frac{\epsilon_C\max_{s\in S}(q(s,\pi_C(s))-\min_{a\in A}q(s,a))}{1-\gamma}.
\end{equation}
In order to finish the proof it remains to show that:
\begin{equation}
0 \leq \mu_E^TV^{\hat{\pi}_C}_{\hat{R}^C}-\mu_E^TV^{\pi_E}_{\hat{R}^C}\leq \frac{\epsilon_C\max_{s\in S}(q(s,\pi_C(s))-\min_{a\in A}q(s,a))+C_1\|\epsilon^C_R\|_{\infty}+\|\epsilon^E_R\|_{\infty}}{1-\gamma}.
\end{equation}
As $\hat\pi_C$ is optimal for $\hat R^C$, we get $\mu_E^TV^{\hat{\pi}_C}_{\hat{R}^C}-\mu_E^TV^{\pi_E}_{\hat{R}^C}\geq 0$. We also notice that:
\begin{equation}
\mu_E^T(V^{\hat{\pi}_C}_{\hat{R}^C}-V^{\pi_E}_{\hat{R}^C})=\mu_E^T(V^{\hat{\pi}_C}_{\hat{R}^C}-V^{\hat{\pi}_C}_{R^C}+V^{\hat{\pi}_C}_{R^C}-V^{\pi_E}_{R^C}+V^{\pi_E}_{R^C}-V^{\pi_E}_{\hat{R}^C}).
\end{equation}
It is very easy to see that:$\mu_E^T(V^{\hat{\pi}_C}_{\hat{R}^C}-V^{\hat{\pi}_C}_{R^C})\leq\frac{C_1\|\epsilon^E_C\|_{\mu_E,1}}{1-\gamma}$ and $\mu_E^T(V^{\pi_E}_{R^C}-V^{\pi_E}_{\hat{R}^C})\leq\frac{\|\epsilon^E_R\|_{\mu_E,1}}{1-\gamma}$.

Indeed via the definition of $C_1$ ($I$ is the identity matrix):
\begin{align}
\mu_E^T(V^{\hat{\pi}_C}_{\hat{R}^C}(s)-V^{\hat{\pi}_C}_{R^C}(s))&=\mu_E^T(I-\gamma P_{\hat{\pi}_C})^{-1}\epsilon^E_C
\\
&=\mu_E^T(\sum_{t\geq0}\gamma^tP_{\hat{\pi}_C}^t)\epsilon^E_C\leq\frac{C_1\|\epsilon^E_C\|_{\mu_E,1}}{1-\gamma}.
\end{align}
And, thanks to the lemma \ref{lemma: calculs V} $\mu_E^T(V^{\pi_E}_{R^C}-V^{\pi_E}_{\hat{R}^C})=\mu_E^T(V^{\pi_E}_{\epsilon_R})=\frac{\mu_E^T\epsilon^E_R}{1-\gamma}\leq\frac{\|\epsilon^E_R\|_{\mu_E,1}}{1-\gamma}$. Thus:
\begin{equation}
\label{equation: cas3-1}
\mu_E^T(V^{\hat{\pi}_C}_{\hat{R}^C}-V^{\hat{\pi}_C}_{R^C}+V^{\pi_E}_{R^C}-V^{\pi_E}_{\hat{R}^C})\leq\frac{C_1\|\epsilon^E_C\|_{\mu_E,1}+\|\epsilon^E_R\|_{\mu_E,1}}{1-\gamma}.
\end{equation}
It remains to deal with the term $\mu_E^T(V^{\hat{\pi}_C}_{R^C}-V^{\pi_E}_{R^C})=\mu_E^T(V^{\hat{\pi}_C}_{R^C}-V^{\pi_C}_{R^C}+V^{\pi_C}_{R^C}-V^{\pi_E}_{R^C})$.
As $\pi_C$ is optimal for the reward $R^C$ then $\mu_E^T(V^{\hat{\pi}_C}_{R^C}-V^{\pi_C}_{R^C})\leq 0$ so:
\begin{equation}
\label{equation: cas3-2}
\mu_E^T(V^{\hat{\pi}_C}_{R^C}-V^{\pi_E}_{R^C})\leq \mu_E^T(V^{\pi_C}_{R^C}-V^{\pi_E}_{R^C})\leq \frac{\epsilon_C\max_{s\in S}(q(s,\pi_C(s))-\min_{a\in A}q(s,a))}{1-\gamma}.
\end{equation}
Finally by regrouping the results in \eqref{equation: cas3-1} and \eqref{equation: cas3-2}:
\begin{equation}
\mu_E^TV^{\hat{\pi}_C}_{\hat{R}^C}-\mu_E^TV^{\pi_E}_{\hat{R}^C}\leq \frac{\epsilon_C\max_{s\in S}(q(s,\pi_C(s))-\min_{a\in A}q(s,a))+C_1\|\epsilon^E_C\|_{\mu_E,1}+\|\epsilon^E_R\|_{\mu_E,1}}{1-\gamma}.
\end{equation}
\end{proof}

  \begin{figure}[!Ht]
  \begin{tabular}{ccc}
  \subfigure[State-of-the-art approaches on the GridWorld]{\includegraphics[width=.45\linewidth]{"Fig1"}\label{Fig1.fig}}&\hspace{.05\linewidth}&
  \subfigure[Our new approach on the GridWorld]{\includegraphics[width=.45\linewidth]{"Fig2"}\label{Fig2.fig}}\\
  \subfigure[Results on the dsriving simulator]{\includegraphics[width=.4\linewidth]{"Fig3"}\label{Fig3.fig}}&\hspace{.05\linewidth}&\includegraphics[width=0.45\linewidth]{"Legend"}
  \end{tabular}
  \caption{Data is shown with mean, standard deviation, minimum and maximum value over 50 runs. The lower baseline is an agent trained on a reward generated by using the same features as the Cascading approach and the classifier, but with a random vector of parameters.}
  \end{figure}
* Experimental results
\label{section: experiments}
#+begin_comment
   - [X] Structured margin (en donnant le QP et en précisant qu'on utilise le sous gradient sans rentrer dans les détails (ref)). Tweaks = Bof.
   - [X] Least squares
   - [X] Heuristic
#+end_comment
   As a classifier, we choose a structured large margin approach \cite{taskar2005learning} which solves :
\begin{equation}
  \min_{\omega,\zeta}\frac{1}{2}\|\omega\|^2 +
  \frac{\eta}{N}\sum_{i=1}^N \zeta_i \text{~~~~s.t.~~~~} \forall i,
  \omega^T{\phi}(s_i,a_i)+\zeta_i \geq \max_a \omega^T
  {\phi}(s_i,a) + \mathcal L (s_i,a). \label{eq:qp_taskar}
\end{equation}
With $\mathcal L(s_i,a_i)=0$ and $\mathcal L(s_i,a\neq a_i)=1$, using some feature function $\phi$ over the state-action space. Practically, we use a subgradient descent on an objective function in which the constraints have been moved \cite{ratliff2006maximum}. When comparing our algorithm to pure classification we used the output of this classifier.

As we try to devise a reward function $\hat R^C$ using /only/ transitions from the expert (case not covered in the anlysis), we have $D_C = D_R$. A good reward function can not be learnt from this data alone as no datapoint exists for tuples $(s,a,s')$ such that $a \neq \pi_E(s)$. We can use a heuristic to get a more complete dataset for the regression step ; assuming (this is quite pessimistic) that deviating from the expert's choice is the worst thing to do, we define $\hat r_{min} = \min\limits_{1\geq i\geq D=D'} \hat r_i - \epsilon, \epsilon>0$ and create the augmented dataset $\{(s_{i},a_{i},\hat{r}_i)\}_{1\leq i \leq D=D'} \cup \{(s_{i},a,\hat{r}_{min})\}_{1\leq i \leq D=D',a\in A,a\neq a_i}$ which is then fed to a simple least-squares regressor using the same features $\phi$ as the classifier. Using straightforward matricial notations, this can be written $\theta = (\Phi^T\Phi + \lambda Id)^{-1}\Phi^T\hat R$.
Finally, we have $\hat R^C(s,a) = \theta^t \phi(s,a)$.

** Results on the GridWorld
#+begin_comment
   - [X] Desc of problems
   - [X] Results
   - [X] Better than state-of-the-art
#+end_comment
   We first tested our approach on a simple $5\times 5$ gridworld. The expert and the agent can choose between 4 actions (down, left right, or up) that sends the player in the corresponding neighbooring cell with probabilty $0.7$. Three times out of ten however, the actions fails and the player moves in another different random direction. Trying to go off the grid will result in the player staying in its position. The expert begins in the lower left corner and is trained to go to the higher right corner by an exact dynamic programming algorithm. The original reward (with respect to which the performance of the agents is computed) is null everywhere but in the higher right corner where it is $1$ for all actions.

The results are shown in two figures. Figure \ref{Fig1.fig} show the baselines we compare ourselves with. Pure classification and the algorithm of \cite{abbeel2004apprenticeship} are on par with one another, both far better than training an agent on a random reward ; their means both converge to a value close to the expert's given enough samples. Notice that the IRL algorithm can still critically fail even when given a lot of samples. This explain the standard deviation going over the maximum. Figure \ref{Fig2.fig} shows results of our algorithm together with the mean performance of both forementionned approaches (variance and min/max were dropped for legibility's sake). It shows the soundness of our cascading approach : when only a few samples are available, we provide a reward that lead to a better (and safer, notice the decreasing standard deviation) policy. When samples are widely available all approaches converge to the same value.

** Results on the highway
#+begin_comment
   - [X] Desc of poblem
   - [X] results
   - [ ] Regression step is useful
#+end_comment
To illustrate a more challenging problem with an increased state space size, we coded a driving simulator inspired from a benchmark seen in \cite{syed2008apprenticeship,syed2008game}. The agent controls a car that can switch between the three lanes of the road, go off-road on either side and modulate between three speed levels. The expert is trained to go as fast as possible (high reward) while avoiding collisions (harshly penalised) with randomly placed slower moving vehicles and avoiding going off-road (moderately penalised). Any other situation receives a null reward. Because the forward RL problem is too hard to solve with so little information, the algorithm from \cite{abbeel2004apprenticeship} was not able to solve the problem using only samples from the expert as ours did. We still used it as a baseline, allowing it to use features expectations and intermediates optimal policies /computed perfectly with the help of the model/.

We used features stemming from a discretization of the state-action space. The performance is assessed with respect to the uniform distribution $\mathcal{U}$, which shows the generalization ability of each method).

As expected, pure classification is not sample efficient enough to provide a good control whereas our approach is able to generalize from very little data and exhibits performance on par with the /fully-informed/ state-of-the-art algorithm when given as few as 100 samples. This empirically shows that the usefulness of the regression step. Furthermore, the state-of-the-art algorihtm make use of intermediate policies, which is computationally costly particularily when the state space gets big. Our approach computational cost scales with the number of samples we are given, which is as we demonstrated a small number. Experimentally, when producing the data for our graph, it took less time to run the cascading approach 50 times for 6 abcissa than to run \cite{abbeel2004apprenticeship}'s algorithm once.

* Related work
\label{section: related work}

First introduced in \cite{russell1998learning} and formalized in \cite{ng2000algorithms}, IRL has sometimes been seen as a way to appreticeship learning \cite{abbeel2004apprenticeship} with algorithms outputing a policy matching a measure of the expert's distribution in the state space. See \cite{neu2009training} for a survey.

Except for \cite{dvijotham2010inverse} which is suitable to linearly solvable MDPs only and for \cite{boularias2011relative} which still requires sampling trajectories from a non expert policy, all IRL algorithm repeatedly solve the forward RL problem, which is a costly procedure information-wise as well as computationnaly.

Classification and IRL have met in the past in \cite{ratliff2006maximum}, but the labels were complete optimal policies rather than actions and the inputs were MDPs. Building on the non trivial notion of metric in an MDP, \cite{melo2010learning} built a kernel for a classification algorithm.


* Conclusion
  
  The proposed approach of cascading two supervised learning algorithms (a classifier and a regressor) is able to learn a reward function from observed behavior for which the expert is provably near optimal. Going further than the analysis, experiments showed the approach (with the help of a simple heuristics) to be sample efficient enough to work with samples from the expert only, giving better results and being more computationnaly efficient than a state-of-the-art algorithm. Our approach will be confronted to real world robotics problems in a near future.

\bibliographystyle{plain}
\bibliography{Biblio}
