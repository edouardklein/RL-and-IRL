#+TITLE: Reducing the dimentionality of the reward space in the Inverse Reinforcement Learning problem
#+AUTHOR: \IEEEauthorblockN{Edouard Klein$^{12}$}\IEEEauthorblockA{$^1$Equipe ABC,\\LORIA-CNRS, France} \and \IEEEauthorblockN{Matthieu Geist$^2$}\IEEEauthorblockA{$^2$Sup\'elec,\\IMS Research group, France} \and \IEEEauthorblockN{Olivier Pietquin$^{23}$}\IEEEauthorblockA{$^3$UMI 2958\\GeorgiaTech-CNRS, France}

#+begin_src emacs-lisp :results silent :exports none
(unless (find "IEEp" org-export-latex-classes :key 'car
         :test 'equal)
  (add-to-list 'org-export-latex-classes
	       '("IEEE"
		 "\\documentclass{IEEEtran}
                  [NO-DEFAULT-PACKAGES]"
		 ("\\section{%s}" . "\\section*{%s}")
		 ("\\subsection{%s}" . "\\subsection*{%s}")
		 ("\\subsubsection{%s}" . "\\subsubsection*{%s}")
		 ("\\paragraph{%s}" . "\\paragraph*{%s}")
		 ("\\subparagraph{%s}" . "\\subparagraph*{%s}")))
  )
 #+end_src
#+EXPORT_EXCLUDE_TAGS: code
#+LaTeX_CLASS: IEEE
#+LaTeX_CLASS_OPTIONS: [conference]
#+LATEX_HEADER: \usepackage{blkarray}
#+LATEX_HEADER: \usepackage{graphicx}
#+LATEX_HEADER: \usepackage{amsthm}
#+LATEX_HEADER: \usepackage{amsfonts}
#+LATEX_HEADER: \usepackage{subfigure}
#+LATEX_HEADER: \usepackage{tabularx}
#+LaTeX_HEADER: \newtheorem{theorem}{Theorem}
#+LaTeX_HEADER: \newtheorem{definition}{Definition}
#+LaTeX_HEADER: \newtheorem{lemma}{Lemma}
#+OPTIONS: toc:nil


#+begin_abstract
This paper deals with the Inverse Reinforcement Learning framework, whose purpose is to learn control policies from demonstrations by an expert. This method inferes from demonstrations the utility function the expert is alledgedly maximizing. We present a proposition that maps the reward space into a subset of smaller dimensionality without loss of generality for all /Markov Decision Processes/ (MDPs). We then present three experimental results showing both the promising aspect of the application of this theorem to existing IRL methods and its shortcomings. We conclude with considerations on further research.
#+end_abstract

#+begin_LaTeX
\IEEEpeerreviewmaketitle
#+end_LaTeX

* Introduction
  In the /Reinforcement Learning/ (RL) framework \cite{sutton1998reinforcement}, an agent is left to find the behavior that, in the long run, maximizes a cumulative reward provided by some oracle. By correctly defining the reward, one can use the RL framework to make an agent fulfil a certain task (without explicitly specifying the task).\\

  For some tasks, defining the corresponding reward is a daunting process. Surely, one has at least an /idea/ about what states are preferable to others. This is however not enough to precisely define the numerical reward that will yield the desired behavior. An example of such a task is driving a car. It is preferable not to be too close to the car in front of ours. Similarily it is preferable not to brake too hard in order to avoid being rear-ended. But exactly how much is it preferable not to break too hard over not being too close to the car in front of ours ? Variations between the relative weights of these two criteria in the reward will certainly lead to very different driving behaviors.\\

  A proposed workaround is the /Inverse Reinforcement Learning/ (IRL) method \cite{ng2000algorithms}, where one tries to learn the reward optimized by an expert\footnote{The expert may be non human, in which one hopes to use a slow or costly expert system to train a less costly agent.}. Then, the /idea/ about what states are preferable to others can be empirically used by a human expert driving the car. The reward function can be infered from the data of the demonstration, and the agent can mimic the expert by optimizing this reward.\\

  The paper is organized as follows: we begin by introducing the necessary mathematical background and presents the related work in the field. We then present and prove our proposition, and explain its use in a Linear Programming framework. Finally, we explain the preliminary experimental work we have done and show where we are heading in light of these preliminary results.
* Background
  Often, the RL problem is solved in a /Markov Decision Process/ setting: the agent is said to be in a state $s_t\in S$ (states respect the Markovian criterion: they contain all the information the agent needs to take a decision) in which it has to choose an action $a\in A$. This will make it step stochastically to another state $s_{t+1}$, receiving a reward $R(s_t)$.\\

  The reward is seen here as a mapping from the states only to the reals. This is not normalized in the literature, some authors using a mapping $S \times S \rightarrow \mathbb{R}$ (e.g., in \cite{ng1999policy}) or a mapping $S \times A \times S \rightarrow \mathbb{R}$. We argue that one should not take the action into account when computing the reward. First because the action is irrelevant, what the reward judges is the consequences of an action. No matter the means, the result is what is important. If an action is risky and has yield a high reward by chance, this will likely not happen again, and the reward need not be lowered by the taking of a risky action as an upcoming transition will likely give the information that taking this action may not be a good idea. Second, in the case of IRL, recording the action can be difficult. It is possible to record the state an agent is in from an external point of view, but guessing the action from the outside is a supplementary engineering problem we shall get rid of if we can. As transitions follow each others, we also argue that there is no use in taking both states of the transition into account. The following state will be present in the next transition, so the associated reward can be computed then. Plus, needing both states to compute the reward is a sign that the states may not respect the markovian criterion. $R$ is represented as a state indexed column vector.\\

  Actions are mapped whith the probability with which they make the agent step from one state to the other. Formally, to each action $a$ is associated a $|S|\times |S|$ matrix $P_a$ whose element $(i,j)$ is the probability to transit to state $j$ when taking action $a$ in state $i$. A policy is a mapping $\pi\textrm{, } s\in S\mapsto \pi(s) \in A$ which can be defined the same way, that is to say by associating it with a $P_\pi$ matrix whose $i$-th line is the $i$-th line of the $(P_a, a=\pi(s))$ matrix. We will from now on speak about actions and policies using indiscriminately  the notation $a$ and $\pi$ or $P_a$ and $P_\pi$.\\

  Each policy can be associated with its /value function/, a mapping $S\rightarrow \mathbb{R}$ which associates to each state the expected sum of discounted rewards an agent will get by following policy $\pi$ from this state. Formally, given a policy $\pi$, the value function is defined by: 
\begin{equation}
V^\pi(s) = E\left[\left.\sum\limits_t \gamma^tR(s_t)\right|s_0=s,\pi\right]
\end{equation}
 and represented, as the reward, by a state indexed column vector.\\

  The RL problem can be casted as finding the /optimal policy/ (often noted $\pi^*$), the value function of which is greater, for every state, than the value function of any other policy: $\forall \pi, V^{\pi^*} \succeq V^\pi$. The value function of any policy is the fixed point of the bellman evaluation operator $T^\pi$ defined as $T^\pi V=R+\gamma P_{\pi}V$.\\

  The IRL problem is the following, given an optimal policy $\pi^*$ and the set of actions $A$ from which the policy was drawn, guess the reward with respect to which this policy is optimal. The reward being the unknown, we will sometimes add the reward in the notation of the value function. Thus, $V^\pi_R$ is the value function of policy $\pi$ under reward $R$.\\

  We denote the set of the optimal policies with respect to a certain reward $R$ and a certain discount factor $\gamma$ as $\Pi^*(R) = \left\{\pi^* | \forall \pi, V^{\pi^*}_R\succeq  V^{\pi}_R\right\}$.\\

  Throughout this paper, we will use the transitive notion of equivalence between rewards: 
  #+begin_definition
  Two rewards $R_1$ and $R_2$ are said to be /equivalent/ if $\Pi^*(R_1)$ = $\Pi^*(R_2)$. This is noted $R_1 \equiv R_2$.
  #+end_definition
  
  A reward is said to be degenerative when it is equivalent to the null vector ($R\equiv[0\dots 0]^T$). Indeed the null vector admits any policy as an optimal policy.
* Related work
  The IRL problem was first defined in \cite{russell1998learning}, where its ill-posed nature was already noted: the set of reward with respect to which the expert's policy is optimal is far from being a singleton. Furthermore there exists some degenerative rewards that admit every policy as optimal and such bear no useful information.\\

  The work of \cite{russell1998learning} was further pushed in \cite{ng2000algorithms}, giving a theorem which defines a sufficient and necessary condition for rewards to be solution of the IRL problem. The problem of the non uniqueness and possible degenerativeness of the solution was worked around using a criterion according to which the difference between the value of the expert's actions and the value of the next-to-best actions is maximized. A penalty term rewarding sparse solutions is also introduced. For large (or continuous) problems, a criterion for sampled trajectories is proposed.\\

  The seminal work of (\cite{abbeel2004apprenticeship}) proposes an iterative algorithm, where the difference between the value of the expert and the value of the second best policy is maximized. Further work (partially summed up in \cite{neu2009training}) often used the same iterative structure, changing the argument that allows to find a unique solution. In \cite{syed2008game}, \cite{syed2008apprenticeship} and \cite{boularias2011bootstrapping}, the authors use a game theoretic approach, in \cite{ratliff2006maximum}, \cite{ratliff2007boosting} and \cite{ratliff2007imitation} the IRL problem is casted as a multiclass classification problem whereas in \cite{neu2007apprenticeship} and \cite{neu2009training} the reward is computed using gradient methods so that the agent's behavior matches the expert's observed behavior. Finally, Bayesian methods have been proposed ; the work in \cite{ramachandran2007bayesian} being very similar to previous work in \cite{chajewska2001learning}, which was not cast as an IRL problem. Follow ups include \cite{dimitrakakis2011bayesian} and \cite{rothkopf2008modular}. Maximum entropy priors are introduced in \cite{ziebart2008maximum}, \cite{boularias2011relative} and \cite{aghasadeghi2011maximum}.

  This paper gives a proposition about reward shaping and begins to explore its potential use in the framework of \cite{ng2000algorithms}. We do not explore any new cost function, the references given above cover that ground extensively. We do present some preliminary experimental results that pave the way for future research, aiming at speeding up search in the reward space or defining more precisely the notion of reward sparsity.
* Dimensionality reduction
** Theorem
   In this subsection, we will show that there exists a set of dimension $|S|-2$ so that every non degenerative reward is equivalent to at least one element of the set.\\

#+begin_lemma
\label{alpha.lemma}
Let $R_1$ be a reward vector, \\
let $R_2$ be a reward vector so that $\exists \alpha > 0, R_2=\alpha R_1$ \\
the following holds:  $R_1\equiv R_2$
#+end_lemma
#+begin_lemma
\label{lambda.lemma}
Let $R_1$ be a reward vector, \\
let $\mathbf{1}$ be the column vector whose $|S|$ elements are all equal to $1$,\\
let $R_2$ be a reward vector so that $\exists \lambda \in \mathbb{R}, R_2= R_1 + \lambda\mathbf{1}$,\\
the following holds:  $R_1\equiv R_2$
#+end_lemma

   Pointers for the proof of this can be found in \cite{puterman1994markov}.

   #+begin_theorem
   Let $M = \{R|\mathbf{1}^TR =  0, ||R||_1 = 1\}$ be,\\
   the following holds: $\forall R \in \mathbb{R}^{|S|}\setminus \{ \lambda \mathbf{1}, \lambda \in \mathbb{R}\}, \exists R'\in M, R'\equiv R$.
   #+end_theorem
   
   This means that the search for the reward can take place in the unit sphere intersected with and hyperplane of the reward space, thus leading to a dimensionality reduction of 2.

   The proof goes as follow: by defining $R' = \alpha(R+\lambda\mathbf{1})$, with $\lambda = -{\mathbf{1}^TR\over |S|}$ and $\alpha = {1\over ||R+\lambda\mathbf{1}||_1}$, one can see that $R'\in M$ and $R' \equiv R$.
** Linear programming constraints
   In \cite{ng2000algorithms}, we are given a necessary and sufficient condition for a reward $R$ to admit a given policy as optimal: for every action $P_a$ the expert following $P_\pi$ had the possibility to take, the following matrix inequality must be met: 
   \begin{equation}
   \label{ng2000algorithms.eqn}
   (P_\pi-P_{a})(I-\gamma P_\pi)^{-1}R\succeq 0
   \end{equation}

   Although the proof can be found in the forementionned paper, we find useful to recall its main argument here: this inequalities stem from the fact that for every action $a$, the expected value of the next state must be less than or the same as the expected value of the next state for taking action $\pi(s)$. This is written as $P_\pi V^\pi \succeq P_a V^\pi$. Consequently, the $i$-th line of the $(P_\pi-P_{a})(I-\gamma P_\pi)^{-1}$ matrix is a constraint on $R$ that, if satisfied, will make it preferable to choose action $\pi(i)$ over $a$ in state $i$.\\
 
   Equation \ref{ng2000algorithms.eqn} yelds at most $|A|\cdot |S| - |S| = (|A|-1)|S|$ constraints. There is $|A|$ matrices $P_a$, each yelding $|S|$ constraints. $|S|$ of these, however, are null because once for each state we will have $a=\pi(s)$ and the resulting line will be filled with zeros.\\

   When a cost function is added, this is a linear programming problem. The constraints stemming from the definition of $M$, i.e. that $\mathbf{1}^TR=0$ and $||R||_1=1$, can be added to it quite easily, thus restricting the solutions to the previously defined $|S|-2$-dimensional subset.\\
** Sparsity of the reward vector
   Sparsity is often stated to be a quality of the reward vector, for example one of the cost functions given in \cite{ng2000algorithms} includes a regularization term ($P(i)$ denotes the $i$-th row of $P$): 
   \begin{equation}
   \label{J.eqn}
   J(R) = \left(\sum_{i=1}^{|S|}\min_{a\in A}(P_\pi(i) - P_a(i))(I - \gamma P_\pi)^{-1} R\right) - \lambda||R||_1
   \end{equation}
* Preliminary work
  In the previous section, no cost function has been provided, as this is beyond the scope of this paper. If anything cost functions provided in \cite{ng2000algorithms} (e.g., the one given Equation \ref{J.eqn}) would yeld good results with our LP formulation above.\\

  A first experiment was run to see if a sparse reward could be found by modifying the simplex algorithm to work without a cost function, but enumerating only sparse rewards. In the simplex algorithm, one start by selecting a /basic feasible solution/ that is to say a vertex of the polytope defined by the linear constraints. Then, one jumps from vertex to vertex by minimizing (or maximizing) the cost function. Basic feasible solutions can be found by choosing which constraints are binding and which are not. The linear system is then solved for the free variables (the variables corresponding to the non binding constraints), if a solution exists, it is a basic feasible solution.\\

  As we did not make use of a cost function, we looked for sparse basic feasible solutions directly. As the constraints are inequalities, the system is loaded with slack variables. If all the slacks variables are considered free, then one only needs two additional variables to get a symetric linear system. Only a few of them are solvable, they are sparse as only two components of the reward vector are non zero.\\ 

  This has proved successful on the now classical gridworld problem, see Fig. \ref{slacksfreeR3.fig}. In this setting, the expert goes from the lower left corner $(0,4)$ of a $5\times 5$ gridworld to its upper right corner $(4,0)$. The true reward function the expert has been trained with is $0$ everywhere but in the upper right corner where the reward is $1$. The reward found by our algorithm just adds a negative reward at the starting point that does not change the behavior. It is sparse. However, with is complexity of $O(|S|^5)$ this algorithm is not very practical.\\

#+begin_LaTeX
\begin{figure}
%\hspace{-1.2cm}
\begin{center}
\includegraphics[width=0.4\textwidth]{../TT_5x5_R3.pdf}
\end{center}
\caption{Reward found by our algorithm on the classic gridworld problem. this is very similar to what can be found in \cite{ng2000algorithms} or \cite{jin2010gaussian}.}
\label{slacksfreeR3.fig}
\end{figure}
#+end_LaTeX
#+begin_LaTeX
\begin{figure}
\begin{center}
\includegraphics[width=0.3\textwidth]{Pi_E.png}
\end{center}
\caption{Policy of the expert. This policy is found by a dynamic programming algorithm when the reward is $0$ everywhere except in the upper right where it is $1$. Actions are executed noisily : with probability $0.3$ another action than what the agent chooses is executed.}
\label{Pi_E.fig}
\end{figure}
#+end_LaTeX

  This success on the gridworld can not be generalized to all MDPs. It is easy to create MDPs where the constraints are so that there is no sparse basic feasible solution that explains the expert's behavior. Randomly generating the transition probabilities and the reward on a 4-states MDP will yield one such counter-example quite quickly.
  
* Further work
  
  We have room for improvement in the computational complexity of the algorithm. The $O(|S|^5)$ complexity mentioned above is the worst-case complexity. The mean complexity can be vastly reduced by solving only the solvable systems and detecting the unsolvable ones earlier in the algorithm. We can also preprocess the constraints to eliminate redundancy.\\

  The fact that our algorithm does not solve all kinds of MDPs is problematic. We have two ideas to circumvent this. The first idea is to characterize the class of MDPs our algorithm is able to work with. By restricting ourselves to a certain class of MDPs we could provide theoretical guarantees about our method. This could also help to reduce the computational complexity by allowing a quicker preprocessing of the linear constraints.\\

  The second idea is to transform the state space so that a sparse basic feasible solution always exist. Although more difficult, this would be more powerful as we could tackle any kind of MDPs. In the gridworld, the expert is going from somewhere to somewhere else and the topology bends itself quite well to a configuration where there is one attractive state and one repulsive state. This is the kind of configuration our algorithm outputs. Some problems however do not present this kind of topology. The balancing pole problem is typical example. The expert is trying to balance a pole with one degree of freedom in the vertical position. There is one attractive state (the vertical position) but there are two repulsive states as the pole can fall on one side or the other. This is certainly problematic for our algorithm, but a state space transformation that would bend the state space so that both repuslive states are close to each other would solve this problem. Our hope is to find some kind of automatic feature discovery mechanism that could do this.\\

  One last track for future work is sampling. Small, discrete state spaces are fine for testing purpose. We should be able to tackle large or continuous problems. The policy of the expert is then unknown but observable. Our algorithm can be modified to work with sample transitions from the expert. Methods exist in the linear programming framework to work with sampled constraints, they could be adapted to our setting.\\

* Conclusion
In this paper, we give a proposition and an experimental results about it. Although promising on a cetain light, there still are serious shortcomings before this can be applied in a practical IRL algorithm. Directions for further work in order to remove these shortcomings are explained.
#+begin_LaTeX
\bibliographystyle{IEEEtran}
\bibliography{../../Biblio/Biblio}
#+end_LaTeX
* Code :code:
  Comme on va mettre un dessin de la politique, voici un petit script python qui extrait la politique d'une matrice et l'affiche d'une mani√®re jolie.
  
    #+begin_src python :tangle ShowPpi.py
from numpy import *
import scipy
from a2str import*
from TT_DP import*

P_north = zeros((25,25))
P_east = zeros((25,25))
P_south = zeros((25,25))
P_west = zeros((25,25))

for a in range(0,4):
    P_a = zeros((25,25))
    for x in range(0,5):
        for y in range(0,5):
            index = x+5*y
            x_north = x
            y_north = 0
            if( y != 0 ):
                y_north = y-1
            index_north = x_north + 5*y_north
                
            x_south = x
            y_south = 4
            if( y != 4 ):
                y_south = y+1
            index_south = x_south + 5*y_south

            y_west = y
            x_west = 0
            if( x != 0 ):
                x_west = x-1
            index_west = x_west + 5*y_west

            y_east = y
            x_east = 4
            if( x != 4 ):
                x_east = x+1
            index_east = x_east + 5*y_east

            main_i = -1
            others = [-1,-1,-1]
            filename = "stderr"
            if( a == 0 ):
                main_i = index_north
                others = [index_south,index_west,index_east]
            elif( a == 1):
                main_i = index_east
                others = [index_south,index_west,index_north]
            elif( a == 2):
                main_i = index_south
                others = [index_north,index_west,index_east]
            elif( a == 3):
                main_i = index_west
                others = [index_south,index_north,index_east]
            
            P_a[index,main_i] +=0.7
            for i in others:
                P_a[index,i] +=0.1
            
            if( a == 0 ):
                P_north = P_a.copy()
            elif( a == 1):
                P_east = P_a.copy()
            elif( a == 2):
                P_south = P_a.copy()
            elif( a == 3):
                P_west = P_a.copy()

    if( a == 0 ):
        filename = "TT_5x5_PENorth.mat"
    elif( a == 1):
        filename = "TT_5x5_PEEast.mat"
    elif( a == 2):
        filename = "TT_5x5_PESouth.mat"
    elif( a == 3):
        filename = "TT_5x5_PEWest.mat"
    f = open( filename, "w" )
    f.write( a2str(P_a) )
    f.close()

R = zeros((25,1))
R[4,0] = 1
P_pi = TT_DP( R, (P_north, P_south, P_west, P_east) )
f = open( "TT_5x5_Ppi.mat", "w" )
f.write( a2str(P_pi) )
f.close()

for i in range(0,5):
    for j in range(0,5):
        s = i*5+j
        pi_s = P_pi[s]
        if all(pi_s == P_north[s]):
            print "^ ",
        elif all(pi_s == P_south[s]):
            print "v ",
        elif all(pi_s == P_west[s]):
            print "< ",
        elif all(pi_s == P_east[s]):
            print "> ",
        else:
            exit(-1)
    print ""


    #+end_src
  
